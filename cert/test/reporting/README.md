# Reporting

This folder contains scripts for the extraction of JSON reports from devices, conversion of JSON reports to CSV, uploading CSV reports to gcloud (for use by Kolab notebooks), and a "scratch" file for experimenting with `matplotlib` for rendering charts from CSV.

## Setup

The reporting scripts require `python3.7x` and `pipenv` to manage dependencies.

``` bash
#(glinux) install dev dependencies for matplotlib
# see here for other platforms: https://pygobject.readthedocs.io/en/latest/getting_started.html
sudo apt install libgirepository1.0-dev gcc libcairo2-dev pkg-config python3-dev gir1.2-gtk-3.0

# install pipenv
pip3 install --user pipenv

# load the dependencies
pipenv install

# load the environment
pipenv shell
```

After installation, pipenv may warn that you need to add its path to your `$PATH` - do so. Then run `pipenv install` from this directory to install dependencies. Finally, after doing so you can run `pipenv shell` to get a local virtualenv in your terminal, or if using VSCode or PyCharm, select the virtualenv matching this folder.

---

## Running the pipeline

### `run.py` 
The entrypoint for the pipeline is `run.py` , which consumes a "recipe" YAML file. Usage is simple: `python run.py --recipe whatever.yaml` . The recipe file structure is as follows:

``` yaml
build:
  # if true, the APK will be cleaned, then built
  clean: false 
  # if provided, the configuration json will be inserted into the APK
  # this is a convenient way to set up a specific set of suites to run
  configuration: configurations/affinity_test.json

systrace:
  # if enabled, a systrace will be collected and merged into the output csv data
  enabled: false

  # systrace categories to record; at present only applies 
  # to local deployments, not ftl
  categories: "sched freq idle am wm gfx view binder_driver hal dalvik input res"

  # only the systrace lines which contain the following keywords will be included
  keywords:

    - MonitorOperation

deployment:

  # rules for local (attached USB) deployment
  # optional; if left out no local run will be performed
  local:
    # if true, the test will be run on all attached devices, this
    # overrides - device_ids: below
    all_attached_devices: false

    # if all_attached_devices is false, manually specify device ids to
    # run the test on. These IDs are sourced via `adb devices` 
    device_ids:

      - 94CX1Z4AJ

  # rules for testing on firebase test lab
  # optional; if left out no ftl run will be performed
  ftl:
    # which test from -args: to execute
    test: test-robo

    # flags: this maps directly to the `--flags-file` argument 
    # when performing ftl deployment
    flags:
      # the gcloud/firebase project id
      project: gamesdk-testing
      # format for logging
      format: json

    # args: this maps directly to the arguments yaml file provided
    # as final argument when performing an ftl deployment
    args:
      test-robo:
        app: ../AndroidCertTest/app/build/outputs/apk/debug/app-debug.apk
        type: game-loop
        timeout: 1500 # 25 minutes
        device:

            - {model: taimen, version: 27}
            - {model: walleye, version: 28}
            - {model: blueline, version: 28}

      test-robo-onlypixel:
        include: [test-robo]
        device:

            - {model: sailfish, version: 28}

# instructions for rendering chart data from collected test results
chart:
  # if false no charting will be done
  enabled: true

  # list of suites to render results for; if empty all suites will be rendered
  suites:

    - "Affinity Test"

```

---

## Helper scripts

### `json_to_csv` 

``` bash
# helper script to batch convert report json files to csv
# this will create a csv file sigbling to each json report
python json_to_csv.py path/to/your/report/jsons
```

### `chart.py` 

``` bash
# display a chart for the provided csv file
python chart.py  --chart --csv path/to/csv.py

# just run an analysis pass on the provided csv files
python chart.py  --analysis --batch path/to/csvs

```

---

## The Report format

The reports generated by `AndroidCertTest` are "JSON-like", but not JSON. Each line of the report is a single valid compact (not-pretty-printed) JSON blob.
The first line is always the build info, which is simply a dict of build information key/value pairs, e.g.:

``` json
{"ID":"QP1A.190711.020.C3","DISPLAY":"QP1A.190711.020.C3","PRODUCT":"blueline","DEVICE":"blueline","BOARD":"blueline","MANUFACTURER":"Google","BRAND":"google","MODEL":"Pixel 3","BOOTLOADER":"b1c1-0.2-5672671","HARDWARE":"blueline","BASE_OS":"","CODENAME":"REL","INCREMENTAL":"5869620","RELEASE":"10","SDK_INT":29,"PREVIEW_SDK_INT":0,"SECURITY_PATCH":"2019-09-05","OPENGLES":"3.2"}
```

Each subsequent line of the report is a "Datum", which is a standardized JSON payload describing:

* `suite_id` : The test suite that was executing when this datum was written
* `operation_id` : The operation that emitted the datum
* `cpu_id` : The CPU on which the datum's information was sourced
* `thread_id` : The thread on which the datum's information was sourced
* `timestamp` : The time in millis when the datum's information was sourced
* `custom` : A custom payload of domain-specific information for the operation

``` json
{"cpu_id":6,"custom":{},"operation_id":"MemoryAllocOperation","suite_id":"Memory allocation","thread_id":"539982699856","timestamp":108342777}
```

The `custom` field might look like so:

``` json
// "custom" field contents for MemoryAllocOperation
{"sys_mem_info":{"available_memory":2427056128,"low_memory":false,"native_allocated":24254280,"oom_score":16},"timestamp_millis":108342777,"total_allocation_bytes":10485760,"total_allocation_mb":10.0}
```

